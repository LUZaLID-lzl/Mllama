#### **1. 准备本地数据集**

- 数据集格式可以是 **纯文本**、**CSV** 或 **JSON**，例如：
    
    ```text
    问题: Ollama如何安装？
    回答: 在Linux上使用apt安装，Windows下载安装包。
    
    问题: 如何查看Ollama运行的模型？
    回答: 使用命令 `ollama ps`。
    ```
    

#### **2. 安装依赖库**

```bash
pip install langchain sentence-transformers faiss-cpu llama-index  # 轻量级工具链
```

### **步骤3：加载数据集并分块**

#### **目标**

将本地数据集切分为适合检索的小片段（避免文本过长导致检索不精准）。

#### **代码示例**

```python
from langchain.document_loaders import TextLoader, CSVLoader, JSONLoader  # 根据文件类型选择
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 假设数据集是文本文件（data.txt），格式为问答对：
# 问题: xxx
# 回答: yyy
# 问题: zzz
# 回答: www

# 1. 加载数据
loader = TextLoader("data.txt")  # 替换为你的文件路径
documents = loader.load()  # 返回 Document 对象列表

# 2. 分块（关键参数说明）
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,   # 每个文本块的最大长度（字符数）
    chunk_overlap=50, # 块之间的重叠部分（避免截断关键信息）
    separators=["\n\n", "\n", "。", "？", "！"]  # 按自然段落或句子切分
)
texts = text_splitter.split_documents(documents)

# 查看分块结果
print(f"切分后共 {len(texts)} 个文本块")
print("示例文本块：", texts[0].page_content)
```

#### **适配不同文件格式**

- **CSV文件**（假设列名为 `question` 和 `answer`）：
    
    ```python
    loader = CSVLoader(
        file_path="data.csv",
        csv_args={"fieldnames": ["question", "answer"]}
    )
    ```
    
- **JSON文件**（假设每条数据为 `{"q": "...", "a": "..."}`）：
    
    ```python
    loader = JSONLoader(
        file_path="data.json",
        jq_schema=".[] | {question: .q, answer: .a}"
    )
    ```
    

---

### **步骤4：构建向量数据库**

#### **目标**

将文本转换为向量（Embedding）并存储，以便快速检索相似内容。

#### **代码示例**

```python
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

# 1. 选择嵌入模型（轻量级，适合本地运行）
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-zh-v1.5",  # 中文小模型，占用约400MB内存
    model_kwargs={"device": "cpu"}         # 无GPU时使用CPU
)

# 2. 生成向量数据库
vector_db = FAISS.from_documents(
    texts,          # 步骤3切分后的文本块
    embeddings      # 嵌入模型
)

# 3. 保存到本地（后续可直接加载，无需重复生成）
vector_db.save_local("faiss_index")  # 保存到 faiss_index 文件夹

# 加载已有数据库（可选）
# vector_db = FAISS.load_local("faiss_index", embeddings)
```

#### **关键参数说明**

|参数/模型|说明|
|---|---|
|`model_name`|可选模型：`all-MiniLM-L6-v2`（英文）、`BAAI/bge-small-zh-v1.5`（中文）|
|`device`|`"cuda"`（GPU加速）或 `"cpu"`（默认）|
|`FAISS`|轻量级向量数据库，适合本地运行|

---

### **步骤5：检索与生成回答**

#### **目标**

根据用户提问检索相关内容，并让大模型生成最终回答。

#### **代码示例**

```python
from langchain.chains import RetrievalQA
from langchain.llms import LlamaCpp  # 使用量化模型（需提前下载GGUF文件）

# 1. 加载本地大模型（以DeepSeek-R1的4-bit量化版为例）
llm = LlamaCpp(
    model_path="deepseek-r1-Q4_K_M.gguf",  # 从HuggingFace下载GGUF文件
    temperature=0.3,     # 控制随机性（0-1，值越小回答越确定）
    max_tokens=200,      # 生成的最大token数
    n_ctx=2048,          # 上下文窗口大小
    n_gpu_layers=20      # 使用GPU加速的层数（CPU设为0）
)

# 2. 创建检索增强链
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vector_db.as_retriever(search_kwargs={"k": 3}),  # 检索前3个相关片段
    chain_type="stuff",  # 简单拼接检索结果
    return_source_documents=True  # 返回检索到的原文
)

# 3. 用户提问
query = "如何查看Ollama运行的模型？"
result = qa_chain({"query": query})

# 4. 输出结果
print("回答：", result["result"])
print("\n参考来源：")
for doc in result["source_documents"]:
    print("-", doc.page_content[:100] + "...")  # 显示片段前100字符
```

#### **关键参数说明**

|参数|说明|
|---|---|
|`search_kwargs={"k":3}`|检索最相关的3个文本块（增大 `k` 可提高答案丰富性，但可能引入噪声）|
|`temperature`|值越小回答越保守，值越大越有创造性（建议0.1-0.5）|
|`chain_type="stuff"`|简单拼接检索结果，其他选项：`"map_reduce"`（复杂任务，但更慢）|
---
### **补充说明**

1. **模型文件准备**：
    
    - 从HuggingFace下载量化后的GGUF模型（例如：TheBloke/deepseek-r1-GGUF

- - ）。
    - 确保模型路径与代码中的 `model_path` 一致。
- **运行资源需求**：
    
    - **CPU模式**：需要至少8GB内存（处理中文时可能需更多）。
    - **GPU加速**：显存需大于4GB（推荐使用 `n_gpu_layers=20` 参数）。
- **自定义提示词**：  
    若需优化回答格式，可添加提示词模板：
    

```python
from langchain.prompts import PromptTemplate

template = """
根据以下上下文回答问题：
{context}

问题：{question}
回答：
"""
qa_chain.combine_documents_chain.llm_chain.prompt = PromptTemplate.from_template(template)
```


---

### **完整流程图**

```markdown
用户提问 → 检索本地数据集 → 拼接上下文 → 大模型生成回答
```

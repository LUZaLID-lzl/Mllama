### **一、常见数据集格式**

#### 1. **JSON/JSONL 格式**（最灵活）

- **适用场景**：结构化数据，每条样本包含多个字段（如指令、输入、输出、标签等）。
- **示例**：
    
    ```json
    // 单条样本示例（单轮对话）
    {
      "instruction": "写一首关于春天的诗",
      "input": "",
      "output": "春风拂面花满枝，燕子归来报晓时..."
    }
    
    // 多轮对话示例
    {
      "conversations": [
        {"role": "user", "content": "你好！"},
        {"role": "assistant", "content": "你好，有什么可以帮助你的？"}
      ]
    }
    ```
    
- **文件扩展名**：`.json`（单文件）或 `.jsonl`（每行一个JSON对象）。

#### 2. **CSV 格式**（适合表格型数据）

- **适用场景**：简单任务（如分类、单轮问答）。
- **示例**：
    
    ```csv
    instruction,input,output
    翻译成英文,今天的天气很好,"The weather is nice today."
    总结以下文本,"人工智能是未来...","人工智能将改变社会..."
    ```
    

#### 3. **纯文本格式**（简单但需自定义处理）

- **适用场景**：无结构化需求的文本生成（如小说、文章续写）。
- **示例**：
    
    ```text
    Instruction: 写一个科幻故事的开头
    Response: 在22世纪的火星殖民地，人类发现了外星文明的遗迹...
    
    Instruction: 解释量子力学
    Response: 量子力学是研究微观粒子行为的物理学分支...
    ```
    

#### 4. **对话格式**（多轮对话）

- **适用场景**：聊天机器人、多轮交互任务。
- **示例**（JSON或列表嵌套）：
    
    ```json
    [
      {"role": "user", "content": "推荐一部科幻电影"},
      {"role": "assistant", "content": "《星际穿越》非常经典。"},
      {"role": "user", "content": "它的导演是谁？"},
      {"role": "assistant", "content": "克里斯托弗·诺兰。"}
    ]
    ```
    

---

### **二、数据预处理步骤**

无论格式如何，最终需将数据转换为模型输入输出对的 **tokenized 格式**。以下是通用预处理代码示例：

#### 1. **加载数据集**

```python
from datasets import load_dataset

# 加载JSON文件（假设每条样本是单轮对话）
dataset = load_dataset("json", data_files="your_data.json")
```

#### 2. **定义预处理函数**

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-r1")

def preprocess_function(examples):
    # 拼接指令和输入（假设字段为 instruction 和 input）
    inputs = []
    for instr, inp in zip(examples["instruction"], examples["input"]):
        if inp.strip() != "":
            inputs.append(f"Instruction: {instr}\nInput: {inp}\nResponse: ")
        else:
            inputs.append(f"Instruction: {instr}\nResponse: ")

    # 对输入进行分词
    model_inputs = tokenizer(
        inputs,
        max_length=512,      # 根据模型最大长度调整
        truncation=True,
        padding="max_length",
        return_tensors="pt"
    )

    # 对输出进行分词（作为标签）
    labels = tokenizer(
        examples["output"],
        max_length=512,
        truncation=True,
        padding="max_length",
        return_tensors="pt"
    )["input_ids"]

    # 将标签添加到输入字典
    model_inputs["labels"] = labels
    return model_inputs
```

#### 3. **应用预处理**

```python
tokenized_dataset = dataset.map(preprocess_function, batched=True)
```

---

### **三、关键注意事项**

1. **数据质量**：
    
    - 确保文本无噪声（如乱码、重复内容）。
    - 输出需与输入强相关（避免答非所问）。
2. **数据划分**：
    
    - 将数据集分为 **训练集**（80-90%）和 **验证集**（10-20%）。
3. **多样性**：
    
    - 覆盖目标任务的不同场景和表达方式（避免过拟合）。
4. **格式一致性**：
    
    - 字段名称（如 `instruction`、`input`、`output`）需与预处理代码匹配。

---

### **四、示例完整流程**

假设数据为 **JSON 格式**，文件内容如下（`data.json`）：

```json
[
  {
    "instruction": "写一首关于春天的诗",
    "input": "",
    "output": "春风拂面花满枝，燕子归来报晓时..."
  },
  {
    "instruction": "翻译成英文",
    "input": "今天的天气很好",
    "output": "The weather is nice today."
  }
]
```

加载并预处理：

```python
from datasets import load_dataset

dataset = load_dataset("json", data_files="data.json")
tokenized_dataset = dataset.map(preprocess_function, batched=True)
tokenized_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
```

最终数据可直接输入模型进行训练！
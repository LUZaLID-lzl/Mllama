from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from app.core.server_config import server_settings
from app.core.model_manager import model_manager
from app.models.knowledge import StreamResponse
import asyncio
from typing import AsyncGenerator
import time
import traceback
import aiohttp
import json

class LLMService:
    def __init__(self):
        self.log_separator = "="*50
        self.max_retries = 3
        self.base_url = "http://localhost:11434"  # Ollama é»˜è®¤åœ°å€
        
    def log(self, message: str, level: str = "INFO", stream: bool = False):
        """æ ¼å¼åŒ–æ—¥å¿—è¾“å‡º"""
        timestamp = time.strftime('%H:%M:%S')
        mode = "[æµå¼]" if stream else "[æ™®é€š]"
        prefix = {
            "INFO": "ğŸ“",
            "WARN": "âš ï¸",
            "ERROR": "âŒ",
            "SUCCESS": "âœ…",
            "THINKING": "ğŸ¤”",
            "SEARCH": "ğŸ”",
            "LLM": "ğŸ¤–",
            "OUTPUT": "ğŸ“¤"
        }.get(level, "")
        print(f"[{timestamp}] {mode} {prefix} {message}")

    async def generate_stream(self, prompt: str) -> AsyncGenerator[str, None]:
        """ç›´æ¥è°ƒç”¨ Ollama API è¿›è¡Œæµå¼ç”Ÿæˆ"""
        timeout = aiohttp.ClientTimeout(
            total=None,
            connect=30,
            sock_read=300
        )
        
        headers = {
            "Content-Type": "application/json",
        }
        
        data = {
            "model": server_settings.LLM_MODEL,
            "prompt": prompt,
            "stream": True,
            "options": {
                "temperature": server_settings.TEMPERATURE,
                "top_p": server_settings.TOP_P,
                "num_ctx": server_settings.NUM_CTX,
                "repeat_penalty": server_settings.REPEAT_PENALTY
            }
        }
        
        retries = 0
        while retries < self.max_retries:
            try:
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.post(
                        f"{self.base_url}/api/generate",
                        headers=headers,
                        json=data
                    ) as response:
                        async for line in response.content:
                            if not line:
                                continue
                            try:
                                chunk = json.loads(line)
                                if chunk.get("done"):
                                    break
                                if "response" in chunk:
                                    yield chunk["response"]
                            except json.JSONDecodeError:
                                continue
                return
                
            except asyncio.TimeoutError:
                retries += 1
                if retries < self.max_retries:
                    self.log(f"ç”Ÿæˆè¶…æ—¶ï¼Œæ­£åœ¨é‡è¯• ({retries}/{self.max_retries})...", "WARN", stream=True)
                    await asyncio.sleep(1)
                else:
                    raise Exception("ç”Ÿæˆå¤šæ¬¡è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•")
            except Exception as e:
                raise Exception(f"ç”Ÿæˆå¤±è´¥: {str(e)}")

    async def query_stream(self, question: str) -> AsyncGenerator[StreamResponse, None]:
        """æµå¼æŸ¥è¯¢çŸ¥è¯†åº“"""
        start_time = time.time()
        
        try:
            # 1. æ£€æŸ¥æ¨¡å‹çŠ¶æ€
            if not model_manager.qa_chain:
                self.log("æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œå°è¯•åˆå§‹åŒ–...", "WARN", stream=True)
                if not await model_manager.initialize():
                    raise Exception("æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")

            # 2. å¼€å§‹æ£€ç´¢
            self.log("å¼€å§‹æ–‡æ¡£æ£€ç´¢...", "SEARCH", stream=True)
            retrieval_start = time.time()
            
            try:
                docs = model_manager.vector_db.similarity_search(
                    question,
                    k=server_settings.TOP_K
                )
                
                if not docs:
                    yield StreamResponse(
                        type="error",
                        content="æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£"
                    )
                    return
                    
            except Exception as e:
                self.log(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {str(e)}", "ERROR", stream=True)
                raise

            retrieval_time = time.time() - retrieval_start
            self.log(f"æ–‡æ¡£æ£€ç´¢å®Œæˆ (ç”¨æ—¶ {retrieval_time:.2f}ç§’)", "SUCCESS", stream=True)

            # 3. å‘é€æºæ–‡æ¡£
            for i, doc in enumerate(docs):
                if model_manager.should_stop:
                    yield StreamResponse(type="stopped", content="æ€è€ƒå·²åœæ­¢")
                    return
                
                yield StreamResponse(
                    type="source",
                    content=doc.page_content,
                    index=i
                )

            # 4. æµå¼ç”Ÿæˆå›ç­”
            self.log("å¼€å§‹ç”Ÿæˆå›ç­”...", "LLM", stream=True)
            
            # æ„å»ºæç¤ºè¯
            context = "\n".join(doc.page_content for doc in docs)
            prompt = f"""åŸºäºä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ã€‚å¦‚æœæ— æ³•ä»èµ„æ–™ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·è¯´æ˜æ— æ³•å›ç­”ã€‚

å‚è€ƒèµ„æ–™ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·åˆ†æåç»™å‡ºç­”æ¡ˆï¼š"""

            # ä½¿ç”¨ç›´æ¥æµå¼ç”Ÿæˆ
            try:
                buffer = ""
                async for chunk in self.generate_stream(prompt):
                    if model_manager.should_stop:
                        yield StreamResponse(type="stopped", content="æ€è€ƒå·²åœæ­¢")
                        return
                    
                    buffer += chunk
                    if len(buffer) >= 10 or chunk.endswith(("ã€‚", "!", "?", "ï¼", "ï¼Ÿ", "\n")):
                        yield StreamResponse(
                            type="content",
                            content=buffer
                        )
                        buffer = ""
                
                # å‘é€å‰©ä½™å†…å®¹
                if buffer:
                    yield StreamResponse(
                        type="content",
                        content=buffer
                    )
                    
            except Exception as e:
                self.log(f"ç”Ÿæˆå¤±è´¥: {str(e)}", "ERROR", stream=True)
                raise
                
            # 5. å‘é€å®Œæˆä¿¡å·
            total_time = time.time() - start_time
            yield StreamResponse(
                type="done",
                content=f"å›ç­”å®Œæˆ (æ€»ç”¨æ—¶ {total_time:.2f}ç§’)"
            )
            
        except Exception as e:
            error_msg = f"æŸ¥è¯¢å¤±è´¥: {str(e)}\n{traceback.format_exc()}"
            self.log(error_msg, "ERROR", stream=True)
            yield StreamResponse(type="error", content=error_msg)
            
        finally:
            model_manager.reset_stop_flag() 